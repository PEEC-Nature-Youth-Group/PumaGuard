{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyiMGdjxXn7r"
      },
      "outputs": [],
      "source": [
        "# Use data from Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_data_directory = '/content/drive/MyDrive/lion_no_lion'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuNNFcePLeFM"
      },
      "outputs": [],
      "source": [
        "# Use data from local directory\n",
        "\n",
        "import os\n",
        "base_data_directory = os.path.realpath('../data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGHMl51SoeHw"
      },
      "outputs": [],
      "source": [
        "# Initialize Tensorflow\n",
        "\n",
        "# Imports tensorflow into notebook, which has the Xception model defined inside\n",
        "# it\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "# Try different backends in the following order: TPU, GPU, CPU and use the\n",
        "# first one available\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  distribution_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "  print(f'Running on a TPU w/{tpu.num_accelerators()[\"TPU\"]} cores')\n",
        "except ValueError:\n",
        "  print(\"WARNING: Not connected to a TPU runtime; Will try GPU\")\n",
        "  if tf.config.list_physical_devices('GPU'):\n",
        "    distribution_strategy = tf.distribute.MirroredStrategy()\n",
        "    print(f'Running on {len(tf.config.list_physical_devices(\"GPU\"))} GPUs')\n",
        "  else:\n",
        "    print('WARNING: Not connected to TPU or GPU runtime; Will use CPU context')\n",
        "    distribution_strategy = tf.distribute.get_strategy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMR6dY6_dpIq"
      },
      "outputs": [],
      "source": [
        "# Settings for the different rows in the table\n",
        "\n",
        "# Set the notebook number to run.\n",
        "notebook_number = 4\n",
        "\n",
        "# Load an existing model and its weights from disk (True) or create a fresh new\n",
        "# model (False).\n",
        "load_model_from_file = False\n",
        "\n",
        "# Load previous training history from file (True).\n",
        "load_history_from_file = False\n",
        "\n",
        "# How many epochs to train for.\n",
        "epochs = 300\n",
        "\n",
        "# No changes below this line.\n",
        "if notebook_number == 1:\n",
        "    epochs = 2_100\n",
        "    image_dimensions = (128, 128) # height, width\n",
        "    with_augmentation = False\n",
        "    batch_size = 16\n",
        "    model_version = \"light\"\n",
        "    alpha = 1e-5\n",
        "    lion_directories = [\n",
        "        # f'{base_data_directory}/lion_1',\n",
        "        f'{base_data_directory}/lion',\n",
        "    ]\n",
        "    no_lion_directories = [\n",
        "        # f'{base_data_directory}/no_lion_1',\n",
        "        f'{base_data_directory}/no_lion',\n",
        "    ]\n",
        "elif notebook_number == 2:\n",
        "    epochs = 1_200\n",
        "    image_dimensions = (256, 256) # height, width\n",
        "    with_augmentation = False\n",
        "    batch_size = 32\n",
        "    model_version = \"light\"\n",
        "    lion_directories = [\n",
        "        # f'{base_data_directory}/lion_1',\n",
        "        f'{base_data_directory}/lion',\n",
        "    ]\n",
        "    no_lion_directories = [\n",
        "        # f'{base_data_directory}/no_lion_1',\n",
        "        f'{base_data_directory}/no_lion',\n",
        "    ]\n",
        "elif notebook_number == 3:\n",
        "    epochs = 900\n",
        "    image_dimensions = (256, 256) # height, width\n",
        "    with_augmentation = True\n",
        "    batch_size = 32\n",
        "    model_version = \"light\"\n",
        "    lion_directories = [\n",
        "        f'{base_data_directory}/lion',\n",
        "    ]\n",
        "    no_lion_directories = [\n",
        "        f'{base_data_directory}/no_lion',\n",
        "    ]\n",
        "elif notebook_number == 4:\n",
        "    image_dimensions = (128, 128) # height, width\n",
        "    with_augmentation = False\n",
        "    batch_size = 16\n",
        "    model_version = \"pre-trained\"\n",
        "    lion_directories = [\n",
        "        f'{base_data_directory}/lion_1',\n",
        "    ]\n",
        "    no_lion_directories = [\n",
        "        f'{base_data_directory}/no_lion_1',\n",
        "    ]\n",
        "elif notebook_number == 5:\n",
        "    image_dimensions = (128, 128) # height, width\n",
        "    with_augmentation = False\n",
        "    batch_size = 16\n",
        "    model_version = \"pre-trained\"\n",
        "    lion_directories = [\n",
        "        f'{base_data_directory}/lion',\n",
        "    ]\n",
        "    no_lion_directories = [\n",
        "        f'{base_data_directory}/no_lion',\n",
        "    ]\n",
        "elif notebook_number == 6:\n",
        "    image_dimensions = (512, 512) # height, width\n",
        "    with_augmentation = False\n",
        "    batch_size = 16\n",
        "    model_version = \"pre-trained\"\n",
        "    lion_directories = [\n",
        "        f'{base_data_directory}/lion',\n",
        "        f'{base_data_directory}/cougar',\n",
        "    ]\n",
        "    no_lion_directories = [\n",
        "        f'{base_data_directory}/no_lion',\n",
        "        f'{base_data_directory}/nocougar',\n",
        "    ]\n",
        "else:\n",
        "    raise Exception(f'Unknown notebook {notebook_number}')\n",
        "\n",
        "model_file = f'{base_data_directory}/model_weights_{notebook_number}_{model_version}_{image_dimensions[0]}_{image_dimensions[1]}.keras'\n",
        "history_file = f'{base_data_directory}/model_history_{notebook_number}_{model_version}_{image_dimensions[0]}_{image_dimensions[1]}.pickle'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B9g8GufGvJI"
      },
      "outputs": [],
      "source": [
        "# Copy images to working directory on runtime\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Find image names in Google Drive\n",
        "lion_images = []\n",
        "for lion in lion_directories:\n",
        "    lion_images += glob.glob(os.path.join(lion, '*JPG'))\n",
        "no_lion_images = []\n",
        "for no_lion in no_lion_directories:\n",
        "    no_lion_images += glob.glob(os.path.join(no_lion, '*JPG'))\n",
        "\n",
        "print(f'Found {len(lion_images)} images tagged as `lion`')\n",
        "print(f'Found {len(no_lion_images)} images tagges as `no-lion`')\n",
        "print(f'In total {len(lion_images) + len(no_lion_images)} images')\n",
        "\n",
        "shutil.rmtree('work', ignore_errors=True)\n",
        "os.makedirs('work/lion')\n",
        "os.makedirs('work/no_lion')\n",
        "\n",
        "print(f'Copying images to working directory {os.path.realpath(\"work\")}')\n",
        "for image in lion_images:\n",
        "    shutil.copy(image, 'work/lion')\n",
        "for image in no_lion_images:\n",
        "    shutil.copy(image, 'work/no_lion')\n",
        "print('Copied all images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7fLWrReY--W"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "\n",
        "if model_version == 'pre-trained':\n",
        "    color_mode = 'rgb'\n",
        "else:\n",
        "    color_mode = 'grayscale'\n",
        "print(f'Using color_mode \\'{color_mode}\\'')\n",
        "\n",
        "# Define augmentation layers which are used in some of the runs\n",
        "augmentation_layers = [\n",
        "    tf.keras.layers.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.RandomRotation(0.01),\n",
        "    tf.keras.layers.RandomZoom(0.05),\n",
        "    tf.keras.layers.RandomBrightness((-0.1, 0.1)),\n",
        "    tf.keras.layers.RandomContrast(0.1),\n",
        "    # tf.keras.layers.RandomCrop(200, 200),\n",
        "    # tf.keras.layers.Rescaling(1./255),\n",
        "]\n",
        "\n",
        "def image_augmentation(image):\n",
        "    # Use augmentation if `with_augmentation` is set to True\n",
        "    if with_augmentation:\n",
        "        for layer in augmentation_layers:\n",
        "            image = layer(image)\n",
        "    return image\n",
        "\n",
        "# Create datasets(training, validation)\n",
        "training_dataset, validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    'work',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='both',\n",
        "    # Seed is always the same in order to ensure that we can reproduce the same\n",
        "    # training session\n",
        "    seed=123,\n",
        "    shuffle=True,\n",
        "    image_size=image_dimensions,\n",
        "    color_mode=color_mode,\n",
        ")\n",
        "\n",
        "training_dataset = training_dataset.map(\n",
        "    lambda img, label: (image_augmentation(img), label),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE,\n",
        ")\n",
        "\n",
        "training_dataset = training_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f'image dimensions {image_dimensions}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_ofzAafTdbm"
      },
      "outputs": [],
      "source": [
        "# Plot a few images from the training dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "images = training_dataset.take(1)\n",
        "\n",
        "for image in images.as_numpy_iterator():\n",
        "    print(f'shape {np.shape(image[0][0])}')\n",
        "    plt.figure(figsize=(18, 18))\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(image[0][i].astype(\"uint8\"), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiYNYL8YedJ8"
      },
      "outputs": [],
      "source": [
        "# Define callbacks for training.\n",
        "\n",
        "import pickle\n",
        "\n",
        "class StoreHistory(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.history = {}\n",
        "        self.number_epochs = 0\n",
        "        history_file_exists = os.path.isfile(history_file)\n",
        "        if history_file_exists and load_history_from_file:\n",
        "            print(f'Loading history from file {history_file}')\n",
        "            with open(history_file, 'rb') as f:\n",
        "                self.history = pickle.load(f)\n",
        "                keys = list(self.history.keys())\n",
        "                self.number_epochs = len(self.history[keys[0]])\n",
        "                print(f'Loaded history of {self.number_epochs} previous epochs')\n",
        "                last_output = f'Epoch {self.number_epochs}: '\n",
        "                for key in keys:\n",
        "                    last_output += f'{key}: {self.history[key][-1]:.4f}'\n",
        "                    if key != keys[-1]:\n",
        "                        last_output += ' - '\n",
        "                print(last_output)\n",
        "        else:\n",
        "            print(f'Creating new history file {history_file}')\n",
        "        for key in ['duration', 'accuracy']:\n",
        "            if key not in self.history:\n",
        "                self.history[key] = []\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        keys = list(self.history.keys())\n",
        "        if len(keys) == 0:\n",
        "            self.number_epochs = 0\n",
        "        else:\n",
        "            self.number_epochs = len(self.history[keys[0]])\n",
        "        print(f'Starting new training with {self.number_epochs} previous epochs')\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if not 'batch_size' in self.history:\n",
        "            self.history['batch_size'] = []\n",
        "        self.history['batch_size'].append(batch_size)\n",
        "        for key in logs:\n",
        "          if not key in self.history:\n",
        "              self.history[key] = []\n",
        "          self.history[key].append(logs[key])\n",
        "        with open(history_file, 'wb') as f:\n",
        "            pickle.dump(self.history, f)\n",
        "            print(f'Epoch {epoch + self.number_epochs + 1} history pickled and saved to file')\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=model_file,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "reduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.75,  # New lr = lr * factor.\n",
        "    patience=50,\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    min_lr=1e-8,  # Lower bound on the learning rate.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrypzknU6Pn6"
      },
      "outputs": [],
      "source": [
        "# Create / load training history\n",
        "\n",
        "def get_best_epoch(history, key):\n",
        "    max_value = 0\n",
        "    max_epoch = 0\n",
        "    if key not in history.history or len(history.history[key]) == 0:\n",
        "        return 0, 0, 0, 0, 0\n",
        "    for epoch in range(len(history.history[key])):\n",
        "        value = history.history[key][epoch]\n",
        "        if value >= max_value: # We want the last, best value\n",
        "            max_value = value\n",
        "            max_epoch = epoch\n",
        "    return (history.history['accuracy'][max_epoch],\n",
        "            history.history['val_accuracy'][max_epoch],\n",
        "            history.history['loss'][max_epoch],\n",
        "            history.history['val_loss'][max_epoch],\n",
        "            max_epoch,\n",
        "    )\n",
        "\n",
        "full_history = StoreHistory()\n",
        "\n",
        "best_accuracy, best_val_accuracy, best_loss, best_val_loss, best_epoch = get_best_epoch(full_history, 'accuracy')\n",
        "print(f'Total time {sum(full_history.history[\"duration\"])} for {len(full_history.history[\"accuracy\"])} epochs')\n",
        "print(f'Best epoch {best_epoch} - accuracy: {best_accuracy:.4f} - val_accuracy: {best_val_accuracy:.4f} - loss: {best_loss:.4f} - val_loss: {best_val_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMF0CS62LeFN"
      },
      "outputs": [],
      "source": [
        "# Define the two models.\n",
        "\n",
        "def pre_trained_model():\n",
        "    # Use the Xception model with imagenet weights as base model\n",
        "    base_model = tf.keras.applications.Xception(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(*image_dimensions, 3),\n",
        "    )\n",
        "\n",
        "    print(f'Number of layers in the base model: {len(base_model.layers)}')\n",
        "    print(f'shape of output layer: {base_model.layers[-1].output_shape}')\n",
        "\n",
        "    # We do not want to change the weights in the Xception model (imagenet\n",
        "    # weights are frozen)\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Average pooling takes the 2,048 outputs of the Xeption model and brings\n",
        "    # it into one output. The sigmoid layer makes sure that one output is\n",
        "    # between 0-1. We will train all parameters in these last two layers\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# The light model does not run properly on a TPU runtime. The loss function\n",
        "# results in `nan` after only one epoch. It does work on GPU runtimes though.\n",
        "def light_model():\n",
        "    inputs = tf.keras.Input(shape=(*image_dimensions, 1))\n",
        "\n",
        "    # Entry block\n",
        "    x = tf.keras.layers.Rescaling(1.0 / 255)(inputs)\n",
        "    x = tf.keras.layers.Conv2D(128, 1, strides=2, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    for size in [256, 512, 728]:\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.SeparableConv2D(size, 1, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.SeparableConv2D(size, 1, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "        x = tf.keras.layers.MaxPooling2D(1, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = tf.keras.layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = tf.keras.layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    x = tf.keras.layers.SeparableConv2D(1024, 1, padding=\"same\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = tf.keras.layers.Dropout(0.1)(x)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(1, activation=None)(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwScWWszaZPU"
      },
      "outputs": [],
      "source": [
        "# Build model\n",
        "#\n",
        "# Prepares model so we can run it\n",
        "\n",
        "import os\n",
        "\n",
        "with distribution_strategy.scope():\n",
        "    model_file_exists = os.path.isfile(model_file)\n",
        "    if load_model_from_file and model_file_exists:\n",
        "        os.stat(model_file)\n",
        "        print(f'Loading model from file {model_file}')\n",
        "        model = tf.keras.models.load_model(model_file)\n",
        "        print('Loaded model from file')\n",
        "    else:\n",
        "        print('Creating new model')\n",
        "        if model_version == \"pre-trained\":\n",
        "            print('Creating new Xception model')\n",
        "            model = pre_trained_model()\n",
        "        elif model_version == \"light\":\n",
        "            print('Creating new light model')\n",
        "            model = light_model()\n",
        "        else:\n",
        "            raise Exception(f'unknown model version {model_version}')\n",
        "\n",
        "        if model_version == \"pre-trained\":\n",
        "            model.build(input_shape=(None, *image_dimensions, 3))\n",
        "        else:\n",
        "            model.build(input_shape=(None, *image_dimensions, 1))\n",
        "\n",
        "        print(f'Number of layers in the model: {len(model.layers)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7H5yjxIYRJX"
      },
      "outputs": [],
      "source": [
        "# Compile model\n",
        "\n",
        "with distribution_strategy.scope():\n",
        "    if model_version == 'pre-trained':\n",
        "        print('Compiling pre-trained model')\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy'],\n",
        "        )\n",
        "    elif model_version == 'light':\n",
        "        print('Compiling light model')\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=alpha),\n",
        "            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "            metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
        "        )\n",
        "    else:\n",
        "        raise Exception(f'Unknown model version {model_version}')\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WepSRguwvHl"
      },
      "source": [
        "# Some notes\n",
        "\n",
        "## Accuracy\n",
        "\n",
        "The accuracy ($A$) measures how often a machine learning model correctly predicts the outcome. You can calculate the accuracy by dividing the number of correct predictions by the total number of predictions. In terms of the confusion matrix\n",
        "\n",
        "|                     | Lion (expected)     | no Lion (expected)  |\n",
        "| ------------------- | ------------------- | ------------------- |\n",
        "| Lion (predicted)    | true positive (TP)  | false positive (FP) |\n",
        "| no Lion (predicted) | false negative (FN) | true negative (TN)  |\n",
        "\n",
        "$$\n",
        "A = \\frac{ \\mathrm{TP} + \\mathrm{TN} }{ \\mathrm{TP} + \\mathrm{TN} + \\mathrm{FP} + \\mathrm{FN} }\n",
        "$$\n",
        "\n",
        "## Loss\n",
        "\n",
        "The binary cross entropy (loss) is defined as\n",
        "\n",
        "$$\n",
        "- \\frac{1}{N} \\sum \\left[ y_{i} \\log p_{i} + (1 - y_{i}) \\log (1 - p_{i}) \\right]\n",
        "$$\n",
        "\n",
        "where the sum goes over all images in the dataset, $ y_{i} $ is the expected label of the image (0 for lion, 1 for no lion), and $ p_{i} $ is the predicted label. Note that $ p_{i} $ is the probability that the image is \"no lion\" and $ (1 - p_{i}) $ is the probability that the image shows a lion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mXJzDLBQSmH"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "start_time = datetime.now()\n",
        "print(start_time)\n",
        "history = model.fit(\n",
        "    training_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=[\n",
        "        checkpoint,\n",
        "        reduce_learning_rate,\n",
        "        full_history,\n",
        "    ]\n",
        ")\n",
        "end_time = datetime.now()\n",
        "print(end_time)\n",
        "\n",
        "duration = (end_time - start_time).total_seconds()\n",
        "print(f'This run took {duration} seconds')\n",
        "\n",
        "if 'duration' not in full_history.history:\n",
        "    full_history.history['duration'] = []\n",
        "full_history.history['duration'].append(duration)\n",
        "\n",
        "print(f'total time {sum(full_history.history[\"duration\"])} for {len(full_history.history[\"accuracy\"])} epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj0YWpFSLvIK"
      },
      "outputs": [],
      "source": [
        "# Print some stats of training so far\n",
        "\n",
        "print(f'Total time {sum(full_history.history[\"duration\"])} for {len(full_history.history[\"accuracy\"])} epochs')\n",
        "\n",
        "best_accuracy, best_val_accuracy, best_loss, best_val_loss, best_epoch = get_best_epoch(full_history, 'accuracy')\n",
        "print(f'Best accuracy - epoch {best_epoch} - accuracy: {best_accuracy:.4f} - val_accuracy: {best_val_accuracy:.4f} - loss: {best_loss:.4f} - val_loss: {best_val_loss:.4f}')\n",
        "\n",
        "best_accuracy, best_val_accuracy, best_loss, best_val_loss, best_epoch = get_best_epoch(full_history, 'val_accuracy')\n",
        "print(f'Best val_accuracy - epoch {best_epoch} - accuracy: {best_accuracy:.4f} - val_accuracy: {best_val_accuracy:.4f} - loss: {best_loss:.4f} - val_loss: {best_val_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hkupgkh0bl2"
      },
      "outputs": [],
      "source": [
        "# Plot training progress\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(18, 10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(full_history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(full_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()), 1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(full_history.history['loss'], label='Training Loss')\n",
        "plt.plot(full_history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0, 1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqSkzCMVjL2C"
      },
      "outputs": [],
      "source": [
        "# Load best model from disk.\n",
        "\n",
        "with distribution_strategy.scope():\n",
        "    model = tf.keras.models.load_model(model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSRYLFs40mmm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "images, labels = next(iter(validation_dataset))\n",
        "\n",
        "# Predict the labels for the images\n",
        "predictions = model.predict(images)\n",
        "\n",
        "# Plot the images and their predicted labels\n",
        "plt.figure(figsize=(18, 18))\n",
        "for i in range(9):\n",
        "  plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(images[i].numpy().astype(\"uint8\"), cmap='gray')\n",
        "  plt.title(f\"Predicted: {predictions[i][0]:.2f} ({'lion' if predictions[i][0] < 0.5 else 'no lion'}), Actual: {labels[i]} ({'lion' if labels[i] == 0 else 'no lion'})\")\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXNjWaSOf1dD"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# classification_directory = '/content/drive/MyDrive/lion_no_lion/stable/angle 1'\n",
        "classification_directory = '/content/drive/MyDrive/lion_no_lion/stable/angle 2/Lion'\n",
        "# classification_directory = '/content/drive/MyDrive/lion_no_lion/cougar'\n",
        "# classification_directory = '/content/drive/MyDrive/lion_no_lion/lion'\n",
        "# classification_directory = '/content/drive/MyDrive/lion_no_lion/nocougar'\n",
        "classification_image_files = glob.glob(os.path.join(classification_directory,\"*JPG\"))\n",
        "classifications = []\n",
        "for image_file in classification_image_files:\n",
        "    image_data = tf.keras.utils.load_img(image_file, target_size=image_dimensions)\n",
        "    image_data = tf.keras.utils.img_to_array(image_data)\n",
        "    prediction = model.predict(np.expand_dims(image_data, 0))\n",
        "    classifications.append({})\n",
        "    classifications[-1]['path'] = image_file\n",
        "    classifications[-1]['image'] = image_data\n",
        "    classifications[-1]['prediction'] = prediction[0][0]\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.array(classifications[-1]['image'].astype(\"uint8\")), cmap='gray')\n",
        "    plt.title(f'{(1-classifications[-1][\"prediction\"])*100:.2f}% lion - {classifications[-1][\"prediction\"]*100:.2f}% no_lion')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
